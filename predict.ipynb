{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from wide_resnet import WideResNet\n",
    "from MyModel import MyModel\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# load model and weights\n",
    "img_size = 224\n",
    "#model64 = MyModel(img_size)()\n",
    "#model64.load_weights(os.path.join(\"models\", \"WRN_16_8.h5\"))\n",
    "\n",
    "#img_size32 = 32\n",
    "#model32 = WideResNet(img_size32, depth=16, k=8)()\n",
    "#model32.load_weights(os.path.join(\"../../dataset/checkpoints\", \"model32.hdf5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"wiki\"\n",
    "data = scipy.io.loadmat(os.path.join(\"../../dataset/\", \"{}32.mat\".format(db)))\n",
    "test_images = data[\"image\"]\n",
    "test_genders = data[\"gender\"][0]\n",
    "test_ages = data[\"age\"][0]\n",
    "\n",
    "cols, rows = 4, 3\n",
    "img_num = cols * rows\n",
    "path_root = \"data/{}_crop/\".format(db)\n",
    "img_ids = np.random.choice(len(test_ages), img_num)\n",
    "sub_test_images = test_images[img_ids,:,:,:]\n",
    "\n",
    "# predict\n",
    "results = model32.predict(sub_test_images)\n",
    "predicted_genders = results[0]\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "for i in range(img_num):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(cv2.cvtColor(sub_test_images[i,:,:,:], cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"{}, {}\".format(int(predicted_ages[i]),\n",
    "                              \"F\" if predicted_genders[i][0]>0.5 else \"M\"))\n",
    "    plt.axis('off')\n",
    "plt.savefig(\"result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import face_recognition\n",
    "mypath='test/'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "#images32 = np.empty(len(onlyfiles), dtype=object)\n",
    "#faces32 = np.empty((len(onlyfiles), img_size32, img_size32, 3))\n",
    "images64 = np.empty(len(onlyfiles), dtype=object)\n",
    "faces64 = np.empty((len(onlyfiles), 224, 224, 3))\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   # images32[n] = cv2.imread( join(mypath,onlyfiles[n]))\n",
    "    #images32[n] = cv2.cvtColor(images32[n],cv2.COLOR_BGR2RGB)\n",
    "    #faces32[n, : , : , :] = cv2.resize(images32[n][35:-35,35:-35,:], (img_size32, img_size32))\n",
    "    image = face_recognition.load_image_file(join(mypath,onlyfiles[n]))\n",
    "    face_locations = face_recognition.face_locations(image)[0]\n",
    "    #print(face_locations)\n",
    "    #print(face_locations[0],face_locations[2])\n",
    "    #print(face_locations[3],face_locations[1])\n",
    "    #print(image.shape)\n",
    "    images64[n] = image[face_locations[0]:face_locations[2],face_locations[3]:face_locations[1],:]\n",
    "    faces64[n, : , : , :] = cv2.resize(images64[n], (224, 224))\n",
    "    \n",
    "#results32 = model32.predict(faces32)\n",
    "results64 = model64.predict(faces64)\n",
    "#predicted_genders32 = results32[0]\n",
    "#predicted_genders64 = results64[0]\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "#predicted_ages32 = results32[1].dot(ages).flatten()\n",
    "predicted_ages64 = results64.dot(ages).flatten()\n",
    "\n",
    "for n in range(len(onlyfiles)):\n",
    "    plt.figure()\n",
    "   # plt.imshow(cv2.resize(images32[n], (img_size64, img_size64)))\n",
    "    #print(\"Age \" +onlyfiles[n]+ \" (32x32): \", int(predicted_ages32[n]))\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (32x32): \", \"F\" if predicted_genders32[n][0] > 0.5 else \"M\")\n",
    "    plt.figure()\n",
    "    plt.imshow(images64[n])\n",
    "    print(\"Age \" +onlyfiles[n]+ \" (64x64): \", predicted_ages64[n])\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (64x64): \", \"F\" if predicted_genders64[n][0] > 0.5 else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def generate_data_generator(generator):\n",
    "    for x_batch,y_batch in generator:\n",
    "            ages = np.arange(0, 101).reshape(101, 1)\n",
    "            round_age = [int(pred.dot(ages).flatten()) for pred in y_batch]\n",
    "            yield x_batch, [y_batch, np.array(round_age)]\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    '../../dataset/wiki_crop/new_database/',\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def x_difference(val_generator, X):\n",
    "    print(len(val_generator.classes))\n",
    "    predictions = model64.predict_generator(generate_data_generator(val_generator),steps=int(6769 / 16))\n",
    "    print(\"Done Prediction...\")\n",
    "    ages = np.arange(0, 101).reshape(101, 1)\n",
    "    predicted_age = predictions[0].dot(ages).flatten()\n",
    "    predicted_age_regressed = predictions[1]\n",
    "    \n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    counter3 = 0\n",
    "    #print(predicted_age_regressed)\n",
    "    \n",
    "    avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(val_generator.classes))]\n",
    "    \n",
    "    #print(avg)\n",
    "    summ = 0\n",
    "    for i in range(len(val_generator.classes)):\n",
    "        real_label = val_generator.classes[i]\n",
    "        #print(real_label)\n",
    "        #print(avg[i])\n",
    "        if abs(real_label - predicted_age[i]) <= X:\n",
    "            counter += 1\n",
    "        if abs(real_label - avg[i]) <= X:\n",
    "            counter2 += 1\n",
    "        if abs(real_label - predicted_age_regressed[i]) <= X:\n",
    "            counter3 += 1\n",
    "        \n",
    "        summ += abs(real_label - avg[i])\n",
    "    #return counter / len(val_generator.classes),counter2 / len(val_generator.classes), counter3 / len(val_generator.classes)\n",
    "    return summ / len(val_generator.classes)\n",
    "c1 = x_difference(val_generator,10)\n",
    "\n",
    "print(c1)\n",
    "#print(c2)\n",
    "#print(c3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model64.save_weights(os.path.join(\"models\", \"Tentativa.h5\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "nb_class = 101\n",
    "\n",
    "path_dataset = \"../../dataset/FGNET/images/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ f for f in listdir(path_dataset) if isfile(join(path_dataset,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "x_batch = [cv2.resize(cv2.cvtColor(cv2.imread(path_dataset+img),cv2.COLOR_BGR2RGB),(224,224)) for img in images]\n",
    "\n",
    "y_true = [int(name.split('A')[1][:2]) for name in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(x_batch, y_true))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "x_batch, y_true = zip(*c)\n",
    "\n",
    "x_batch = np.array(x_batch)\n",
    "\n",
    "\n",
    "percentage = 0.2\n",
    "x_data = (x_batch[:int(len(x_batch)*(1-percentage))],x_batch[int(len(x_batch)*(1-percentage)):])\n",
    "y_data = (y_true[:int(len(y_true)*(1-percentage))],y_true[int(len(y_true)*(1-percentage)):])\n",
    "\n",
    "x_train, y_train = x_data[0], np.array(y_data[0])\n",
    "x_val, y_val = x_data[1], np.array(y_data[1])\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(y_val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and val\n",
    "\n",
    "y_train2 = to_categorical(y_train, nb_class).astype('float32')\n",
    "y_val2 = to_categorical(y_val, nb_class).astype('float32')\n",
    "\n",
    "y_train = y_train.reshape(len(y_train),1)\n",
    "y_val = y_val.reshape(len(y_val),1)\n",
    "\n",
    "y_train = np.array(y_train).reshape(-1)\n",
    "y_val = np.array(y_val).reshape(-1)\n",
    "#print(y_train2.shape)\n",
    "#print(x_val.shape)\n",
    "\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training data:  4492\n",
      "Size of Validation data:  1122\n"
     ]
    }
   ],
   "source": [
    "images = [ f for f in listdir(path_dataset) if isfile(join(path_dataset,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "num_val = int(len(images)*0.2)\n",
    "\n",
    "train_names = images[num_val:]\n",
    "val_names = images[:num_val]\n",
    "\n",
    "print(\"Size of Training data: \", len(train_names))\n",
    "print(\"Size of Validation data: \", len(val_names))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def get_batch(list_filenames):\n",
    "    while True:\n",
    "        for batches in range(len(list_filenames) // batch_size):\n",
    "            x_batch = []\n",
    "            real_age = []\n",
    "            categorical_age = []\n",
    "            for filename in list_filenames[batches*batch_size:min(len(list_filenames),(batches+1)*batch_size)]:\n",
    "            #get real age\n",
    "                #age = int(filename.split('A')[1][:-4])\n",
    "                age = int(filename.split('A')[1][:2])\n",
    "                real_age.append(age)\n",
    "                #categorical\n",
    "                cat_age = to_categorical(age,nb_class)\n",
    "                categorical_age.append(cat_age)\n",
    "                #get image, color change and resize -> not needed, organize_db does this\n",
    "                image = cv2.imread(path_dataset+filename)\n",
    "                if image.shape != (224,224,3):\n",
    "                    image = cv2.resize(image,(224,224))\n",
    "                x_batch.append(image)\n",
    "                #print(image.shape)\n",
    "            x_batch = np.array(x_batch)\n",
    "            real_age = np.array(real_age)\n",
    "            categorical_age = np.array(categorical_age)\n",
    "            #print(x_batch.shape)\n",
    "            #print(real_age.shape)\n",
    "            #print(categorical_age.shape)\n",
    "            yield x_batch,[categorical_age,real_age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting...\n",
      "Epoch 1/15\n",
      "70/70 [==============================] - 239s 3s/step - loss: 234.1690 - pred_age_loss: 13.5596 - regress_age_loss: 221.0171 - pred_age_acc: 0.0533 - regress_age_acc: 0.0304 - val_loss: 177.9611 - val_pred_age_loss: 8.6748 - val_regress_age_loss: 167.3451 - val_pred_age_acc: 0.1811 - val_regress_age_acc: 0.0358\n",
      "Epoch 2/15\n",
      "70/70 [==============================] - 237s 3s/step - loss: 158.7388 - pred_age_loss: 10.4951 - regress_age_loss: 147.2581 - pred_age_acc: 0.1259 - regress_age_acc: 0.0404 - val_loss: 126.2947 - val_pred_age_loss: 5.6553 - val_regress_age_loss: 117.2737 - val_pred_age_acc: 0.3438 - val_regress_age_acc: 0.0441\n",
      "Epoch 3/15\n",
      "70/70 [==============================] - 238s 3s/step - loss: 124.1565 - pred_age_loss: 7.2559 - regress_age_loss: 114.3650 - pred_age_acc: 0.1998 - regress_age_acc: 0.0473 - val_loss: 98.2032 - val_pred_age_loss: 2.4494 - val_regress_age_loss: 90.8433 - val_pred_age_acc: 0.4936 - val_regress_age_acc: 0.0551\n",
      "Epoch 4/15\n",
      "70/70 [==============================] - 240s 3s/step - loss: 107.2925 - pred_age_loss: 4.3781 - regress_age_loss: 98.9921 - pred_age_acc: 0.2737 - regress_age_acc: 0.0522 - val_loss: 83.5388 - val_pred_age_loss: 1.0955 - val_regress_age_loss: 76.9035 - val_pred_age_acc: 0.7123 - val_regress_age_acc: 0.0680\n",
      "Epoch 5/15\n",
      "70/70 [==============================] - 238s 3s/step - loss: 97.8157 - pred_age_loss: 2.8590 - regress_age_loss: 90.3201 - pred_age_acc: 0.3757 - regress_age_acc: 0.0536 - val_loss: 75.4198 - val_pred_age_loss: 0.7829 - val_regress_age_loss: 68.9841 - val_pred_age_acc: 0.8511 - val_regress_age_acc: 0.0662\n",
      "Epoch 6/15\n",
      "70/70 [==============================] - 239s 3s/step - loss: 91.7562 - pred_age_loss: 2.1713 - regress_age_loss: 84.6469 - pred_age_acc: 0.4415 - regress_age_acc: 0.0556 - val_loss: 68.5096 - val_pred_age_loss: 0.5114 - val_regress_age_loss: 62.2512 - val_pred_age_acc: 0.9228 - val_regress_age_acc: 0.0699\n",
      "Epoch 7/15\n",
      "70/70 [==============================] - 237s 3s/step - loss: 86.3585 - pred_age_loss: 1.7355 - regress_age_loss: 79.5082 - pred_age_acc: 0.5217 - regress_age_acc: 0.0533 - val_loss: 63.8710 - val_pred_age_loss: 0.3265 - val_regress_age_loss: 57.7457 - val_pred_age_acc: 0.9779 - val_regress_age_acc: 0.0836\n",
      "Epoch 8/15\n",
      "70/70 [==============================] - 237s 3s/step - loss: 81.2557 - pred_age_loss: 1.4665 - regress_age_loss: 74.5801 - pred_age_acc: 0.5748 - regress_age_acc: 0.0583 - val_loss: 59.4934 - val_pred_age_loss: 0.2064 - val_regress_age_loss: 53.4680 - val_pred_age_acc: 0.9917 - val_regress_age_acc: 0.0910\n",
      "Epoch 9/15\n",
      "70/70 [==============================] - 237s 3s/step - loss: 76.7385 - pred_age_loss: 1.2078 - regress_age_loss: 70.2321 - pred_age_acc: 0.6397 - regress_age_acc: 0.0600 - val_loss: 56.0402 - val_pred_age_loss: 0.1195 - val_regress_age_loss: 50.0981 - val_pred_age_acc: 0.9982 - val_regress_age_acc: 0.0919\n",
      "Epoch 10/15\n",
      "69/70 [============================>.] - ETA: 0s - loss: 73.3500 - pred_age_loss: 0.9922 - regress_age_loss: 66.9910 - pred_age_acc: 0.7047 - regress_age_acc: 0.0630- ETA: 2s - loss: 74.0388 - pred_age_loss: 1.0139 - regress_age_loss: 67.6655 - pred_age_acc: 0.7015 - regress_age"
     ]
    }
   ],
   "source": [
    "###\n",
    "from MyModel import MyModel\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "model = MyModel(224,nb_class=nb_class,trainable=False)()\n",
    "sgd = Adam(lr=0.0001)\n",
    "\n",
    "#getBatches\n",
    "\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Start Fitting...\")\n",
    "\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=15, validation_data = get_batch(val_names),validation_steps=len(val_names))\n",
    "sgd = Adam(lr=0.00001)\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=5, validation_data = get_batch(val_names),validation_steps=len(val_names))\n",
    "sgd = Adam(lr=0.000001)\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=5, validation_data = get_batch(val_names),validation_steps=len(val_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"../../dataset/FGNET_TEST/\"\n",
    "\n",
    "images = [ f for f in listdir(path_test) if isfile(join(path_test,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "x_test = [cv2.resize(cv2.cvtColor(cv2.imread(path_fg_net+img),cv2.COLOR_BGR2RGB),(224,224)) for img in images]\n",
    "\n",
    "y_test = [int(name.split('A')[1][:2]) for name in images]\n",
    "\n",
    "y_pred = model.predict(np.array(x_test))\n",
    "\n",
    "#for img in x_test[:10]:\n",
    "#    plt.figure()\n",
    "#    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.arange(0, nb_class).reshape(nb_class, 1)\n",
    "predicted_age = y_pred[0].dot(ages).flatten()\n",
    "predicted_age_regressed = y_pred[1]\n",
    "\n",
    "avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(x_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ1 = 0\n",
    "summ2 = 0\n",
    "summ3 = 0\n",
    "for i in range(len(x_test)):\n",
    "    summ1 += abs(y_test[i] - avg[i])\n",
    "    summ2 += abs(y_test[i] - predicted_age[i])\n",
    "    summ3 += abs(y_test[i] - predicted_age_regressed[i])\n",
    "\n",
    "print(summ1/len(x_test), \" -- AVG\")\n",
    "print(summ2/len(x_test), \" -- CLASS\")\n",
    "print(summ3/len(x_test), \" -- REG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/state_art3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import face_recognition\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(\"predictor/shape_predictor_5_face_landmarks.dat\")\n",
    "\n",
    "mypath='test/'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "#images32 = np.empty(len(onlyfiles), dtype=object)\n",
    "#faces32 = np.empty((len(onlyfiles), img_size32, img_size32, 3))\n",
    "images64 = np.empty(len(onlyfiles), dtype=object)\n",
    "faces64 = np.empty((len(onlyfiles), 224, 224, 3))\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   # images32[n] = cv2.imread( join(mypath,onlyfiles[n]))\n",
    "    #images32[n] = cv2.cvtColor(images32[n],cv2.COLOR_BGR2RGB)\n",
    "    #faces32[n, : , : , :] = cv2.resize(images32[n][35:-35,35:-35,:], (img_size32, img_size32))\n",
    "    image = face_recognition.load_image_file(join(mypath,onlyfiles[n]))\n",
    "    \n",
    "    dets = detector(image, 1)\n",
    "    \n",
    "    if len(dets) == 0:\n",
    "        continue\n",
    "    \n",
    "    faces = dlib.full_object_detections()\n",
    "    for detection in dets:\n",
    "        faces.append(sp(image, detection))\n",
    "    \n",
    "    images = dlib.get_face_chips(image, faces, size=320, padding=0.5)\n",
    "    \n",
    "    images64[n] = images[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(face_locations)\n",
    "    #print(face_locations[0],face_locations[2])\n",
    "    #print(face_locations[3],face_locations[1])\n",
    "    #print(image.shape)\n",
    "    #images64[n] = image[face_locations[0]:face_locations[2],face_locations[3]:face_locations[1],:]\n",
    "    faces64[n, : , : , :] = cv2.resize(images64[n], (224, 224))\n",
    "    \n",
    "#results32 = model32.predict(faces32)\n",
    "y_pred = model.predict(faces64)\n",
    "\n",
    "ages = np.arange(0, nb_class).reshape(nb_class, 1)\n",
    "predicted_age = y_pred[0].dot(ages).flatten()\n",
    "predicted_age_regressed = y_pred[1]\n",
    "\n",
    "avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(onlyfiles))]\n",
    "\n",
    "for n in range(len(onlyfiles)):\n",
    "    plt.figure()\n",
    "   # plt.imshow(cv2.resize(images32[n], (img_size64, img_size64)))\n",
    "    #print(\"Age \" +onlyfiles[n]+ \" (32x32): \", int(predicted_ages32[n]))\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (32x32): \", \"F\" if predicted_genders32[n][0] > 0.5 else \"M\")\n",
    "    plt.figure()\n",
    "    plt.imshow(images64[n])\n",
    "    print(\"AVG \" +onlyfiles[n]+ \" (64x64): \", avg[n])\n",
    "    print(\"Class \" +onlyfiles[n]+ \" (64x64): \", predicted_age[n])\n",
    "    print(\"Regress \" +onlyfiles[n]+ \" (64x64): \", predicted_age_regressed[n])\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (64x64): \", \"F\" if predicted_genders64[n][0] > 0.5 else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
