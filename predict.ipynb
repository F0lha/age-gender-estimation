{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from wide_resnet import WideResNet\n",
    "from MyModel import MyModel\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# load model and weights\n",
    "img_size = 224\n",
    "#model64 = MyModel(img_size)()\n",
    "#model64.load_weights(os.path.join(\"models\", \"WRN_16_8.h5\"))\n",
    "\n",
    "#img_size32 = 32\n",
    "#model32 = WideResNet(img_size32, depth=16, k=8)()\n",
    "#model32.load_weights(os.path.join(\"../../dataset/checkpoints\", \"model32.hdf5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"wiki\"\n",
    "data = scipy.io.loadmat(os.path.join(\"../../dataset/\", \"{}32.mat\".format(db)))\n",
    "test_images = data[\"image\"]\n",
    "test_genders = data[\"gender\"][0]\n",
    "test_ages = data[\"age\"][0]\n",
    "\n",
    "cols, rows = 4, 3\n",
    "img_num = cols * rows\n",
    "path_root = \"data/{}_crop/\".format(db)\n",
    "img_ids = np.random.choice(len(test_ages), img_num)\n",
    "sub_test_images = test_images[img_ids,:,:,:]\n",
    "\n",
    "# predict\n",
    "results = model32.predict(sub_test_images)\n",
    "predicted_genders = results[0]\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "for i in range(img_num):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(cv2.cvtColor(sub_test_images[i,:,:,:], cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"{}, {}\".format(int(predicted_ages[i]),\n",
    "                              \"F\" if predicted_genders[i][0]>0.5 else \"M\"))\n",
    "    plt.axis('off')\n",
    "plt.savefig(\"result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import face_recognition\n",
    "mypath='test/'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "#images32 = np.empty(len(onlyfiles), dtype=object)\n",
    "#faces32 = np.empty((len(onlyfiles), img_size32, img_size32, 3))\n",
    "images64 = np.empty(len(onlyfiles), dtype=object)\n",
    "faces64 = np.empty((len(onlyfiles), 224, 224, 3))\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   # images32[n] = cv2.imread( join(mypath,onlyfiles[n]))\n",
    "    #images32[n] = cv2.cvtColor(images32[n],cv2.COLOR_BGR2RGB)\n",
    "    #faces32[n, : , : , :] = cv2.resize(images32[n][35:-35,35:-35,:], (img_size32, img_size32))\n",
    "    image = face_recognition.load_image_file(join(mypath,onlyfiles[n]))\n",
    "    face_locations = face_recognition.face_locations(image)[0]\n",
    "    #print(face_locations)\n",
    "    #print(face_locations[0],face_locations[2])\n",
    "    #print(face_locations[3],face_locations[1])\n",
    "    #print(image.shape)\n",
    "    images64[n] = image[face_locations[0]:face_locations[2],face_locations[3]:face_locations[1],:]\n",
    "    faces64[n, : , : , :] = cv2.resize(images64[n], (224, 224))\n",
    "    \n",
    "#results32 = model32.predict(faces32)\n",
    "results64 = model64.predict(faces64)\n",
    "#predicted_genders32 = results32[0]\n",
    "#predicted_genders64 = results64[0]\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "#predicted_ages32 = results32[1].dot(ages).flatten()\n",
    "predicted_ages64 = results64.dot(ages).flatten()\n",
    "\n",
    "for n in range(len(onlyfiles)):\n",
    "    plt.figure()\n",
    "   # plt.imshow(cv2.resize(images32[n], (img_size64, img_size64)))\n",
    "    #print(\"Age \" +onlyfiles[n]+ \" (32x32): \", int(predicted_ages32[n]))\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (32x32): \", \"F\" if predicted_genders32[n][0] > 0.5 else \"M\")\n",
    "    plt.figure()\n",
    "    plt.imshow(images64[n])\n",
    "    print(\"Age \" +onlyfiles[n]+ \" (64x64): \", predicted_ages64[n])\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (64x64): \", \"F\" if predicted_genders64[n][0] > 0.5 else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def generate_data_generator(generator):\n",
    "    for x_batch,y_batch in generator:\n",
    "            ages = np.arange(0, 101).reshape(101, 1)\n",
    "            round_age = [int(pred.dot(ages).flatten()) for pred in y_batch]\n",
    "            yield x_batch, [y_batch, np.array(round_age)]\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    '../../dataset/wiki_crop/new_database/',\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def x_difference(val_generator, X):\n",
    "    print(len(val_generator.classes))\n",
    "    predictions = model64.predict_generator(generate_data_generator(val_generator),steps=int(6769 / 16))\n",
    "    print(\"Done Prediction...\")\n",
    "    ages = np.arange(0, 101).reshape(101, 1)\n",
    "    predicted_age = predictions[0].dot(ages).flatten()\n",
    "    predicted_age_regressed = predictions[1]\n",
    "    \n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    counter3 = 0\n",
    "    #print(predicted_age_regressed)\n",
    "    \n",
    "    avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(val_generator.classes))]\n",
    "    \n",
    "    #print(avg)\n",
    "    summ = 0\n",
    "    for i in range(len(val_generator.classes)):\n",
    "        real_label = val_generator.classes[i]\n",
    "        #print(real_label)\n",
    "        #print(avg[i])\n",
    "        if abs(real_label - predicted_age[i]) <= X:\n",
    "            counter += 1\n",
    "        if abs(real_label - avg[i]) <= X:\n",
    "            counter2 += 1\n",
    "        if abs(real_label - predicted_age_regressed[i]) <= X:\n",
    "            counter3 += 1\n",
    "        \n",
    "        summ += abs(real_label - avg[i])\n",
    "    #return counter / len(val_generator.classes),counter2 / len(val_generator.classes), counter3 / len(val_generator.classes)\n",
    "    return summ / len(val_generator.classes)\n",
    "c1 = x_difference(val_generator,10)\n",
    "\n",
    "print(c1)\n",
    "#print(c2)\n",
    "#print(c3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model64.save_weights(os.path.join(\"models\", \"Tentativa.h5\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "nb_class = 101\n",
    "\n",
    "path_dataset = \"../../dataset/FGNET/images/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ f for f in listdir(path_dataset) if isfile(join(path_dataset,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "x_batch = [cv2.resize(cv2.cvtColor(cv2.imread(path_dataset+img),cv2.COLOR_BGR2RGB),(224,224)) for img in images]\n",
    "\n",
    "y_true = [int(name.split('A')[1][:2]) for name in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(x_batch, y_true))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "x_batch, y_true = zip(*c)\n",
    "\n",
    "x_batch = np.array(x_batch)\n",
    "\n",
    "\n",
    "percentage = 0.2\n",
    "x_data = (x_batch[:int(len(x_batch)*(1-percentage))],x_batch[int(len(x_batch)*(1-percentage)):])\n",
    "y_data = (y_true[:int(len(y_true)*(1-percentage))],y_true[int(len(y_true)*(1-percentage)):])\n",
    "\n",
    "x_train, y_train = x_data[0], np.array(y_data[0])\n",
    "x_val, y_val = x_data[1], np.array(y_data[1])\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(y_val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and val\n",
    "\n",
    "y_train2 = to_categorical(y_train, nb_class).astype('float32')\n",
    "y_val2 = to_categorical(y_val, nb_class).astype('float32')\n",
    "\n",
    "y_train = y_train.reshape(len(y_train),1)\n",
    "y_val = y_val.reshape(len(y_val),1)\n",
    "\n",
    "y_train = np.array(y_train).reshape(-1)\n",
    "y_val = np.array(y_val).reshape(-1)\n",
    "#print(y_train2.shape)\n",
    "#print(x_val.shape)\n",
    "\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training data:  4492\n",
      "Size of Validation data:  1122\n"
     ]
    }
   ],
   "source": [
    "images = [ f for f in listdir(path_dataset) if isfile(join(path_dataset,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "num_val = int(len(images)*0.2)\n",
    "\n",
    "train_names = images[num_val:]\n",
    "val_names = images[:num_val]\n",
    "\n",
    "print(\"Size of Training data: \", len(train_names))\n",
    "print(\"Size of Validation data: \", len(val_names))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(list_filenames):\n",
    "    while True:\n",
    "        for batches in range(len(list_filenames) // batch_size):\n",
    "            x_batch = []\n",
    "            real_age = []\n",
    "            categorical_age = []\n",
    "            for filename in list_filenames[batches*batch_size:min(len(list_filenames),(batches+1)*batch_size)]:\n",
    "            #get real age\n",
    "                #age = int(filename.split('A')[1][:-4])\n",
    "                age = int(filename.split('A')[1][:2])\n",
    "                real_age.append(age)\n",
    "                #categorical\n",
    "                cat_age = to_categorical(age,nb_class)\n",
    "                categorical_age.append(cat_age)\n",
    "                #get image, color change and resize -> not needed, organize_db does this\n",
    "                image = cv2.imread(path_dataset+filename)\n",
    "                if image.shape != (224,224,3):\n",
    "                    image = cv2.resize(image,(224,224))\n",
    "                x_batch.append(image)\n",
    "                #print(image.shape)\n",
    "            x_batch = np.array(x_batch)\n",
    "            real_age = np.array(real_age)\n",
    "            categorical_age = np.array(categorical_age)\n",
    "            #print(x_batch.shape)\n",
    "            #print(real_age.shape)\n",
    "            #print(categorical_age.shape)\n",
    "            yield x_batch,[categorical_age,real_age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting...\n",
      "Epoch 1/25\n",
      "140/140 [==============================] - 17s 118ms/step - loss: 186.8788 - pred_age_loss: 13.8343 - regress_age_loss: 173.7368 - pred_age_acc: 0.0654 - pred_age_mean_absolute_error: 0.0185 - regress_age_acc: 0.0350 - regress_age_mean_absolute_error: 9.7270 - val_loss: 118.4857 - val_pred_age_loss: 11.2198 - val_regress_age_loss: 106.8188 - val_pred_age_acc: 0.1366 - val_pred_age_mean_absolute_error: 0.0171 - val_regress_age_acc: 0.0420 - val_regress_age_mean_absolute_error: 7.4007\n",
      "Epoch 2/25\n",
      "140/140 [==============================] - 17s 122ms/step - loss: 119.2059 - pred_age_loss: 11.1197 - regress_age_loss: 107.6596 - pred_age_acc: 0.1337 - pred_age_mean_absolute_error: 0.0172 - regress_age_acc: 0.0498 - regress_age_mean_absolute_error: 7.4795 - val_loss: 82.0171 - val_pred_age_loss: 5.6671 - val_regress_age_loss: 73.2529 - val_pred_age_acc: 0.3393 - val_pred_age_mean_absolute_error: 0.0134 - val_regress_age_acc: 0.0643 - val_regress_age_mean_absolute_error: 6.2260\n",
      "Epoch 3/25\n",
      "140/140 [==============================] - 16s 113ms/step - loss: 98.9681 - pred_age_loss: 6.2179 - regress_age_loss: 89.9663 - pred_age_acc: 0.2344 - pred_age_mean_absolute_error: 0.0154 - regress_age_acc: 0.0518 - regress_age_mean_absolute_error: 6.8430 - val_loss: 68.3164 - val_pred_age_loss: 1.7421 - val_regress_age_loss: 61.5874 - val_pred_age_acc: 0.6250 - val_pred_age_mean_absolute_error: 0.0099 - val_regress_age_acc: 0.0563 - val_regress_age_mean_absolute_error: 5.8037\n",
      "Epoch 4/25\n",
      "140/140 [==============================] - 17s 123ms/step - loss: 87.0964 - pred_age_loss: 3.0906 - regress_age_loss: 79.7235 - pred_age_acc: 0.3509 - pred_age_mean_absolute_error: 0.0137 - regress_age_acc: 0.0538 - regress_age_mean_absolute_error: 6.4949 - val_loss: 60.9848 - val_pred_age_loss: 0.8969 - val_regress_age_loss: 54.7391 - val_pred_age_acc: 0.7946 - val_pred_age_mean_absolute_error: 0.0083 - val_regress_age_acc: 0.0616 - val_regress_age_mean_absolute_error: 5.5209\n",
      "Epoch 5/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 79.3855 - pred_age_loss: 2.0516 - regress_age_loss: 72.5911 - pred_age_acc: 0.4560 - pred_age_mean_absolute_error: 0.0122 - regress_age_acc: 0.0578 - regress_age_mean_absolute_error: 6.1632 - val_loss: 54.9685 - val_pred_age_loss: 0.5133 - val_regress_age_loss: 48.9722 - val_pred_age_acc: 0.9205 - val_pred_age_mean_absolute_error: 0.0059 - val_regress_age_acc: 0.0607 - val_regress_age_mean_absolute_error: 5.2528\n",
      "Epoch 6/25\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 74.8482 - pred_age_loss: 1.5311 - regress_age_loss: 68.3714 - pred_age_acc: 0.5719 - pred_age_mean_absolute_error: 0.0106 - regress_age_acc: 0.0587 - regress_age_mean_absolute_error: 6.0015 - val_loss: 49.8470 - val_pred_age_loss: 0.2847 - val_regress_age_loss: 44.0222 - val_pred_age_acc: 0.9670 - val_pred_age_mean_absolute_error: 0.0038 - val_regress_age_acc: 0.0705 - val_regress_age_mean_absolute_error: 4.9825\n",
      "Epoch 7/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 70.7959 - pred_age_loss: 1.1809 - regress_age_loss: 64.5513 - pred_age_acc: 0.6516 - pred_age_mean_absolute_error: 0.0089 - regress_age_acc: 0.0625 - regress_age_mean_absolute_error: 5.8376 - val_loss: 47.8075 - val_pred_age_loss: 0.1516 - val_regress_age_loss: 42.1065 - val_pred_age_acc: 0.9893 - val_pred_age_mean_absolute_error: 0.0022 - val_regress_age_acc: 0.0652 - val_regress_age_mean_absolute_error: 4.9430\n",
      "Epoch 8/25\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 66.4594 - pred_age_loss: 0.9197 - regress_age_loss: 60.4031 - pred_age_acc: 0.7279 - pred_age_mean_absolute_error: 0.0076 - regress_age_acc: 0.0685 - regress_age_mean_absolute_error: 5.5799 - val_loss: 43.6039 - val_pred_age_loss: 0.0824 - val_regress_age_loss: 37.9957 - val_pred_age_acc: 0.9946 - val_pred_age_mean_absolute_error: 0.0013 - val_regress_age_acc: 0.0696 - val_regress_age_mean_absolute_error: 4.6524\n",
      "Epoch 9/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 63.2010 - pred_age_loss: 0.7627 - regress_age_loss: 57.2823 - pred_age_acc: 0.7708 - pred_age_mean_absolute_error: 0.0065 - regress_age_acc: 0.0674 - regress_age_mean_absolute_error: 5.4684 - val_loss: 40.3448 - val_pred_age_loss: 0.0473 - val_regress_age_loss: 34.8141 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 8.0710e-04 - val_regress_age_acc: 0.0750 - val_regress_age_mean_absolute_error: 4.4410\n",
      "Epoch 10/25\n",
      "140/140 [==============================] - 18s 127ms/step - loss: 60.4560 - pred_age_loss: 0.6144 - regress_age_loss: 54.6722 - pred_age_acc: 0.8183 - pred_age_mean_absolute_error: 0.0056 - regress_age_acc: 0.0670 - regress_age_mean_absolute_error: 5.3033 - val_loss: 37.9417 - val_pred_age_loss: 0.0279 - val_regress_age_loss: 32.4820 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 4.9696e-04 - val_regress_age_acc: 0.0723 - val_regress_age_mean_absolute_error: 4.2885\n",
      "Epoch 11/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 59.1175 - pred_age_loss: 0.5219 - regress_age_loss: 53.4420 - pred_age_acc: 0.8449 - pred_age_mean_absolute_error: 0.0049 - regress_age_acc: 0.0717 - regress_age_mean_absolute_error: 5.2518 - val_loss: 35.7308 - val_pred_age_loss: 0.0164 - val_regress_age_loss: 30.3400 - val_pred_age_acc: 0.9991 - val_pred_age_mean_absolute_error: 3.0081e-04 - val_regress_age_acc: 0.0786 - val_regress_age_mean_absolute_error: 4.1353\n",
      "Epoch 12/25\n",
      "140/140 [==============================] - 18s 125ms/step - loss: 55.3042 - pred_age_loss: 0.4372 - regress_age_loss: 49.7354 - pred_age_acc: 0.8708 - pred_age_mean_absolute_error: 0.0042 - regress_age_acc: 0.0717 - regress_age_mean_absolute_error: 5.0496 - val_loss: 33.6192 - val_pred_age_loss: 0.0138 - val_regress_age_loss: 28.2949 - val_pred_age_acc: 0.9982 - val_pred_age_mean_absolute_error: 2.4700e-04 - val_regress_age_acc: 0.0777 - val_regress_age_mean_absolute_error: 3.9804\n",
      "Epoch 13/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 55.0146 - pred_age_loss: 0.3832 - regress_age_loss: 49.5388 - pred_age_acc: 0.8873 - pred_age_mean_absolute_error: 0.0038 - regress_age_acc: 0.0730 - regress_age_mean_absolute_error: 4.9935 - val_loss: 31.6665 - val_pred_age_loss: 0.0071 - val_regress_age_loss: 26.4126 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 1.3457e-04 - val_regress_age_acc: 0.0911 - val_regress_age_mean_absolute_error: 3.8203\n",
      "Epoch 14/25\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 53.1174 - pred_age_loss: 0.3264 - regress_age_loss: 47.7381 - pred_age_acc: 0.9051 - pred_age_mean_absolute_error: 0.0033 - regress_age_acc: 0.0748 - regress_age_mean_absolute_error: 4.8776 - val_loss: 30.0296 - val_pred_age_loss: 0.0057 - val_regress_age_loss: 24.8455 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 1.0900e-04 - val_regress_age_acc: 0.1098 - val_regress_age_mean_absolute_error: 3.6633\n",
      "Epoch 15/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 51.3460 - pred_age_loss: 0.2744 - regress_age_loss: 46.0631 - pred_age_acc: 0.9232 - pred_age_mean_absolute_error: 0.0029 - regress_age_acc: 0.0746 - regress_age_mean_absolute_error: 4.8311 - val_loss: 29.0238 - val_pred_age_loss: 0.0038 - val_regress_age_loss: 23.9120 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 7.4062e-05 - val_regress_age_acc: 0.1000 - val_regress_age_mean_absolute_error: 3.6208\n",
      "Epoch 16/25\n",
      "140/140 [==============================] - 17s 125ms/step - loss: 48.9420 - pred_age_loss: 0.2507 - regress_age_loss: 43.7432 - pred_age_acc: 0.9241 - pred_age_mean_absolute_error: 0.0027 - regress_age_acc: 0.0862 - regress_age_mean_absolute_error: 4.6899 - val_loss: 27.4790 - val_pred_age_loss: 0.0029 - val_regress_age_loss: 22.4411 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 5.7152e-05 - val_regress_age_acc: 0.1071 - val_regress_age_mean_absolute_error: 3.4952\n",
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 16s 116ms/step - loss: 48.3085 - pred_age_loss: 0.2303 - regress_age_loss: 43.1941 - pred_age_acc: 0.9350 - pred_age_mean_absolute_error: 0.0025 - regress_age_acc: 0.0804 - regress_age_mean_absolute_error: 4.6602 - val_loss: 26.2160 - val_pred_age_loss: 0.0017 - val_regress_age_loss: 21.2537 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 3.3757e-05 - val_regress_age_acc: 0.1161 - val_regress_age_mean_absolute_error: 3.3839\n",
      "Epoch 18/25\n",
      "140/140 [==============================] - 18s 125ms/step - loss: 46.5634 - pred_age_loss: 0.1943 - regress_age_loss: 41.5432 - pred_age_acc: 0.9455 - pred_age_mean_absolute_error: 0.0022 - regress_age_acc: 0.0897 - regress_age_mean_absolute_error: 4.6191 - val_loss: 25.4646 - val_pred_age_loss: 0.0017 - val_regress_age_loss: 20.5798 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 3.3225e-05 - val_regress_age_acc: 0.1205 - val_regress_age_mean_absolute_error: 3.3226\n",
      "Epoch 19/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 45.5139 - pred_age_loss: 0.1834 - regress_age_loss: 40.5774 - pred_age_acc: 0.9482 - pred_age_mean_absolute_error: 0.0021 - regress_age_acc: 0.0846 - regress_age_mean_absolute_error: 4.5265 - val_loss: 24.7025 - val_pred_age_loss: 0.0045 - val_regress_age_loss: 19.8952 - val_pred_age_acc: 0.9982 - val_pred_age_mean_absolute_error: 6.9977e-05 - val_regress_age_acc: 0.1098 - val_regress_age_mean_absolute_error: 3.2588\n",
      "Epoch 20/25\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 44.2569 - pred_age_loss: 0.1656 - regress_age_loss: 39.4088 - pred_age_acc: 0.9565 - pred_age_mean_absolute_error: 0.0019 - regress_age_acc: 0.0873 - regress_age_mean_absolute_error: 4.4674 - val_loss: 23.7457 - val_pred_age_loss: 0.0010 - val_regress_age_loss: 19.0205 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 2.0565e-05 - val_regress_age_acc: 0.1170 - val_regress_age_mean_absolute_error: 3.2009\n",
      "Epoch 21/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 43.5793 - pred_age_loss: 0.1524 - regress_age_loss: 38.8193 - pred_age_acc: 0.9578 - pred_age_mean_absolute_error: 0.0018 - regress_age_acc: 0.0897 - regress_age_mean_absolute_error: 4.4035 - val_loss: 22.9982 - val_pred_age_loss: 0.0014 - val_regress_age_loss: 18.3551 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 2.6597e-05 - val_regress_age_acc: 0.1170 - val_regress_age_mean_absolute_error: 3.1159\n",
      "Epoch 22/25\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 43.5478 - pred_age_loss: 0.1480 - regress_age_loss: 38.8727 - pred_age_acc: 0.9592 - pred_age_mean_absolute_error: 0.0017 - regress_age_acc: 0.0933 - regress_age_mean_absolute_error: 4.3729 - val_loss: 22.3389 - val_pred_age_loss: 0.0010 - val_regress_age_loss: 17.7793 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 2.0620e-05 - val_regress_age_acc: 0.1223 - val_regress_age_mean_absolute_error: 3.0430\n",
      "Epoch 23/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 42.6724 - pred_age_loss: 0.1278 - regress_age_loss: 38.0919 - pred_age_acc: 0.9670 - pred_age_mean_absolute_error: 0.0015 - regress_age_acc: 0.0951 - regress_age_mean_absolute_error: 4.3309 - val_loss: 21.4198 - val_pred_age_loss: 0.0019 - val_regress_age_loss: 16.9454 - val_pred_age_acc: 0.9991 - val_pred_age_mean_absolute_error: 3.1672e-05 - val_regress_age_acc: 0.1259 - val_regress_age_mean_absolute_error: 2.9721\n",
      "Epoch 24/25\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 42.4056 - pred_age_loss: 0.1130 - regress_age_loss: 37.9190 - pred_age_acc: 0.9728 - pred_age_mean_absolute_error: 0.0014 - regress_age_acc: 0.0873 - regress_age_mean_absolute_error: 4.3465 - val_loss: 20.6196 - val_pred_age_loss: 7.4370e-04 - val_regress_age_loss: 16.2332 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 1.4659e-05 - val_regress_age_acc: 0.1179 - val_regress_age_mean_absolute_error: 2.9432\n",
      "Epoch 25/25\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 39.2780 - pred_age_loss: 0.1155 - regress_age_loss: 34.8782 - pred_age_acc: 0.9690 - pred_age_mean_absolute_error: 0.0014 - regress_age_acc: 0.0942 - regress_age_mean_absolute_error: 4.1727 - val_loss: 19.9152 - val_pred_age_loss: 7.7060e-04 - val_regress_age_loss: 15.6172 - val_pred_age_acc: 1.0000 - val_pred_age_mean_absolute_error: 1.5154e-05 - val_regress_age_acc: 0.1187 - val_regress_age_mean_absolute_error: 2.8882\n",
      "Epoch 1/5\n",
      "140/140 [==============================] - 18s 130ms/step - loss: 40.3545 - pred_age_loss: 0.0911 - regress_age_loss: 36.0595 - pred_age_acc: 0.9761 - regress_age_acc: 0.0922 - val_loss: 19.9499 - val_pred_age_loss: 3.8879e-04 - val_regress_age_loss: 15.7245 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1196\n",
      "Epoch 2/5\n",
      "140/140 [==============================] - 16s 115ms/step - loss: 39.6548 - pred_age_loss: 0.0723 - regress_age_loss: 35.4096 - pred_age_acc: 0.9853 - regress_age_acc: 0.0980 - val_loss: 19.5483 - val_pred_age_loss: 2.9779e-04 - val_regress_age_loss: 15.3545 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1116\n",
      "Epoch 3/5\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 38.6342 - pred_age_loss: 0.0701 - regress_age_loss: 34.4198 - pred_age_acc: 0.9824 - regress_age_acc: 0.1036 - val_loss: 19.4030 - val_pred_age_loss: 2.7351e-04 - val_regress_age_loss: 15.2375 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1205\n",
      "Epoch 4/5\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 39.3620 - pred_age_loss: 0.0571 - regress_age_loss: 35.1817 - pred_age_acc: 0.9882 - regress_age_acc: 0.0946 - val_loss: 19.2699 - val_pred_age_loss: 2.5243e-04 - val_regress_age_loss: 15.1316 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1214\n",
      "Epoch 5/5\n",
      "140/140 [==============================] - 17s 125ms/step - loss: 38.0147 - pred_age_loss: 0.0584 - regress_age_loss: 33.8604 - pred_age_acc: 0.9893 - regress_age_acc: 0.0975 - val_loss: 19.2010 - val_pred_age_loss: 2.3155e-04 - val_regress_age_loss: 15.0891 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1268\n",
      "Epoch 1/5\n",
      "140/140 [==============================] - 19s 133ms/step - loss: 36.8731 - pred_age_loss: 0.0527 - regress_age_loss: 32.7409 - pred_age_acc: 0.9882 - regress_age_acc: 0.0989 - val_loss: 19.1460 - val_pred_age_loss: 2.2350e-04 - val_regress_age_loss: 15.0431 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1214\n",
      "Epoch 2/5\n",
      "140/140 [==============================] - 16s 115ms/step - loss: 37.1421 - pred_age_loss: 0.0484 - regress_age_loss: 33.0173 - pred_age_acc: 0.9908 - regress_age_acc: 0.0946 - val_loss: 19.1235 - val_pred_age_loss: 2.1485e-04 - val_regress_age_loss: 15.0247 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1223\n",
      "Epoch 3/5\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 37.2933 - pred_age_loss: 0.0510 - regress_age_loss: 33.1710 - pred_age_acc: 0.9904 - regress_age_acc: 0.0975 - val_loss: 19.1101 - val_pred_age_loss: 2.0607e-04 - val_regress_age_loss: 15.0150 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1187\n",
      "Epoch 4/5\n",
      "140/140 [==============================] - 16s 116ms/step - loss: 37.1415 - pred_age_loss: 0.0495 - regress_age_loss: 33.0235 - pred_age_acc: 0.9906 - regress_age_acc: 0.0955 - val_loss: 19.1009 - val_pred_age_loss: 2.0410e-04 - val_regress_age_loss: 15.0092 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1187\n",
      "Epoch 5/5\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 36.8483 - pred_age_loss: 0.0526 - regress_age_loss: 32.7320 - pred_age_acc: 0.9893 - regress_age_acc: 0.1018 - val_loss: 19.0906 - val_pred_age_loss: 1.9864e-04 - val_regress_age_loss: 15.0021 - val_pred_age_acc: 1.0000 - val_regress_age_acc: 0.1161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93e0154588>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "import keras\n",
    "from MyModel import MyModel\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "model = MyModel(224,nb_class=nb_class,trainable=False)()\n",
    "sgd = Adam(lr=0.0001)\n",
    "\n",
    "#getBatches\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "tbCallBack1 = keras.callbacks.TensorBoard(log_dir='../../dataset/graphs/Graph1', histogram_freq=0, write_graph=True, write_images=True)\n",
    "tbCallBack2 = keras.callbacks.TensorBoard(log_dir='../../dataset/graphs/Graph2', histogram_freq=0, write_graph=True, write_images=True)\n",
    "tbCallBack3 = keras.callbacks.TensorBoard(log_dir='../../dataset/graphs/Graph3', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy','MAE'])\n",
    "\n",
    "print(\"Start Fitting...\")\n",
    "\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=25, validation_data = get_batch(val_names),validation_steps=len(val_names)// batch_size, callbacks=[tbCallBack1])\n",
    "\n",
    "sgd = Adam(lr=0.00001)\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=5, validation_data = get_batch(val_names),validation_steps=len(val_names)// batch_size, callbacks=[tbCallBack2])\n",
    "sgd = Adam(lr=0.000001)\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=5, validation_data = get_batch(val_names),validation_steps=len(val_names)// batch_size, callbacks=[tbCallBack3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "model = load_model('../../dataset/models/state_meh.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "path_test = \"../../dataset/FGNET_TEST/\"\n",
    "\n",
    "images = [ f for f in listdir(path_test) if isfile(join(path_test,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "x_test = [cv2.resize(cv2.cvtColor(cv2.imread(path_test+img),cv2.COLOR_BGR2RGB),(224,224)) for img in images]\n",
    "\n",
    "y_test = [int(name.split('A')[1][:2]) for name in images]\n",
    "\n",
    "y_pred = model.predict(np.array(x_test))\n",
    "\n",
    "#for img in x_test[:10]:\n",
    "#    plt.figure()\n",
    "#    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.arange(0, nb_class).reshape(nb_class, 1)\n",
    "predicted_age = y_pred[0].dot(ages).flatten()\n",
    "predicted_age_regressed = y_pred[1]\n",
    "\n",
    "avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(x_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4268868]  -- AVG\n",
      "3.600441425922433  -- CLASS\n",
      "[4.509377]  -- REG\n"
     ]
    }
   ],
   "source": [
    "summ1 = 0\n",
    "summ2 = 0\n",
    "summ3 = 0\n",
    "for i in range(len(x_test)):\n",
    "    summ1 += abs(y_test[i] - avg[i])\n",
    "    summ2 += abs(y_test[i] - predicted_age[i])\n",
    "    summ3 += abs(y_test[i] - predicted_age_regressed[i])\n",
    "\n",
    "print(summ1/len(x_test), \" -- AVG\")\n",
    "print(summ2/len(x_test), \" -- CLASS\")\n",
    "print(summ3/len(x_test), \" -- REG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../dataset/models/state_art_maybe.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import face_recognition\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(\"predictor/shape_predictor_5_face_landmarks.dat\")\n",
    "\n",
    "mypath='test/'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "#images32 = np.empty(len(onlyfiles), dtype=object)\n",
    "#faces32 = np.empty((len(onlyfiles), img_size32, img_size32, 3))\n",
    "images64 = np.empty(len(onlyfiles), dtype=object)\n",
    "faces64 = np.empty((len(onlyfiles), 224, 224, 3))\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   # images32[n] = cv2.imread( join(mypath,onlyfiles[n]))\n",
    "    #images32[n] = cv2.cvtColor(images32[n],cv2.COLOR_BGR2RGB)\n",
    "    #faces32[n, : , : , :] = cv2.resize(images32[n][35:-35,35:-35,:], (img_size32, img_size32))\n",
    "    image = face_recognition.load_image_file(join(mypath,onlyfiles[n]))\n",
    "    \n",
    "    dets = detector(image, 1)\n",
    "    \n",
    "    if len(dets) == 0:\n",
    "        continue\n",
    "    \n",
    "    faces = dlib.full_object_detections()\n",
    "    for detection in dets:\n",
    "        faces.append(sp(image, detection))\n",
    "    \n",
    "    images = dlib.get_face_chips(image, faces, size=320, padding=0.5)\n",
    "    \n",
    "    images64[n] = images[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(face_locations)\n",
    "    #print(face_locations[0],face_locations[2])\n",
    "    #print(face_locations[3],face_locations[1])\n",
    "    #print(image.shape)\n",
    "    #images64[n] = image[face_locations[0]:face_locations[2],face_locations[3]:face_locations[1],:]\n",
    "    faces64[n, : , : , :] = cv2.resize(images64[n], (224, 224))\n",
    "    \n",
    "#results32 = model32.predict(faces32)\n",
    "y_pred = model.predict(faces64)\n",
    "\n",
    "ages = np.arange(0, nb_class).reshape(nb_class, 1)\n",
    "predicted_age = y_pred[0].dot(ages).flatten()\n",
    "predicted_age_regressed = y_pred[1]\n",
    "\n",
    "avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(onlyfiles))]\n",
    "\n",
    "for n in range(len(onlyfiles)):\n",
    "    plt.figure()\n",
    "   # plt.imshow(cv2.resize(images32[n], (img_size64, img_size64)))\n",
    "    #print(\"Age \" +onlyfiles[n]+ \" (32x32): \", int(predicted_ages32[n]))\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (32x32): \", \"F\" if predicted_genders32[n][0] > 0.5 else \"M\")\n",
    "    plt.figure()\n",
    "    plt.imshow(images64[n])\n",
    "    print(\"AVG \" +onlyfiles[n]+ \" (64x64): \", avg[n])\n",
    "    print(\"Class \" +onlyfiles[n]+ \" (64x64): \", predicted_age[n])\n",
    "    print(\"Regress \" +onlyfiles[n]+ \" (64x64): \", predicted_age_regressed[n])\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (64x64): \", \"F\" if predicted_genders64[n][0] > 0.5 else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
