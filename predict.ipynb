{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from wide_resnet import WideResNet\n",
    "from MyModel import MyModel\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\"\n",
    "# load model and weights\n",
    "img_size = 224\n",
    "#model64 = MyModel(img_size)()\n",
    "#model64.load_weights(os.path.join(\"models\", \"WRN_16_8.h5\"))\n",
    "\n",
    "#img_size32 = 32\n",
    "#model32 = WideResNet(img_size32, depth=16, k=8)()\n",
    "#model32.load_weights(os.path.join(\"../../dataset/checkpoints\", \"model32.hdf5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"wiki\"\n",
    "data = scipy.io.loadmat(os.path.join(\"../../dataset/\", \"{}32.mat\".format(db)))\n",
    "test_images = data[\"image\"]\n",
    "test_genders = data[\"gender\"][0]\n",
    "test_ages = data[\"age\"][0]\n",
    "\n",
    "cols, rows = 4, 3\n",
    "img_num = cols * rows\n",
    "path_root = \"data/{}_crop/\".format(db)\n",
    "img_ids = np.random.choice(len(test_ages), img_num)\n",
    "sub_test_images = test_images[img_ids,:,:,:]\n",
    "\n",
    "# predict\n",
    "results = model32.predict(sub_test_images)\n",
    "predicted_genders = results[0]\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "for i in range(img_num):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(cv2.cvtColor(sub_test_images[i,:,:,:], cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"{}, {}\".format(int(predicted_ages[i]),\n",
    "                              \"F\" if predicted_genders[i][0]>0.5 else \"M\"))\n",
    "    plt.axis('off')\n",
    "plt.savefig(\"result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import face_recognition\n",
    "mypath='test/'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "#images32 = np.empty(len(onlyfiles), dtype=object)\n",
    "#faces32 = np.empty((len(onlyfiles), img_size32, img_size32, 3))\n",
    "images64 = np.empty(len(onlyfiles), dtype=object)\n",
    "faces64 = np.empty((len(onlyfiles), 224, 224, 3))\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   # images32[n] = cv2.imread( join(mypath,onlyfiles[n]))\n",
    "    #images32[n] = cv2.cvtColor(images32[n],cv2.COLOR_BGR2RGB)\n",
    "    #faces32[n, : , : , :] = cv2.resize(images32[n][35:-35,35:-35,:], (img_size32, img_size32))\n",
    "    image = face_recognition.load_image_file(join(mypath,onlyfiles[n]))\n",
    "    face_locations = face_recognition.face_locations(image)[0]\n",
    "    #print(face_locations)\n",
    "    #print(face_locations[0],face_locations[2])\n",
    "    #print(face_locations[3],face_locations[1])\n",
    "    #print(image.shape)\n",
    "    images64[n] = image[face_locations[0]:face_locations[2],face_locations[3]:face_locations[1],:]\n",
    "    faces64[n, : , : , :] = cv2.resize(images64[n], (224, 224))\n",
    "    \n",
    "#results32 = model32.predict(faces32)\n",
    "results64 = model64.predict(faces64)\n",
    "#predicted_genders32 = results32[0]\n",
    "#predicted_genders64 = results64[0]\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "#predicted_ages32 = results32[1].dot(ages).flatten()\n",
    "predicted_ages64 = results64.dot(ages).flatten()\n",
    "\n",
    "for n in range(len(onlyfiles)):\n",
    "    plt.figure()\n",
    "   # plt.imshow(cv2.resize(images32[n], (img_size64, img_size64)))\n",
    "    #print(\"Age \" +onlyfiles[n]+ \" (32x32): \", int(predicted_ages32[n]))\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (32x32): \", \"F\" if predicted_genders32[n][0] > 0.5 else \"M\")\n",
    "    plt.figure()\n",
    "    plt.imshow(images64[n])\n",
    "    print(\"Age \" +onlyfiles[n]+ \" (64x64): \", predicted_ages64[n])\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (64x64): \", \"F\" if predicted_genders64[n][0] > 0.5 else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def generate_data_generator(generator):\n",
    "    for x_batch,y_batch in generator:\n",
    "            ages = np.arange(0, 101).reshape(101, 1)\n",
    "            round_age = [int(pred.dot(ages).flatten()) for pred in y_batch]\n",
    "            yield x_batch, [y_batch, np.array(round_age)]\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    '../../dataset/wiki_crop/new_database/',\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def x_difference(val_generator, X):\n",
    "    print(len(val_generator.classes))\n",
    "    predictions = model64.predict_generator(generate_data_generator(val_generator),steps=int(6769 / 16))\n",
    "    print(\"Done Prediction...\")\n",
    "    ages = np.arange(0, 101).reshape(101, 1)\n",
    "    predicted_age = predictions[0].dot(ages).flatten()\n",
    "    predicted_age_regressed = predictions[1]\n",
    "    \n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    counter3 = 0\n",
    "    #print(predicted_age_regressed)\n",
    "    \n",
    "    avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(val_generator.classes))]\n",
    "    \n",
    "    #print(avg)\n",
    "    summ = 0\n",
    "    for i in range(len(val_generator.classes)):\n",
    "        real_label = val_generator.classes[i]\n",
    "        #print(real_label)\n",
    "        #print(avg[i])\n",
    "        if abs(real_label - predicted_age[i]) <= X:\n",
    "            counter += 1\n",
    "        if abs(real_label - avg[i]) <= X:\n",
    "            counter2 += 1\n",
    "        if abs(real_label - predicted_age_regressed[i]) <= X:\n",
    "            counter3 += 1\n",
    "        \n",
    "        summ += abs(real_label - avg[i])\n",
    "    #return counter / len(val_generator.classes),counter2 / len(val_generator.classes), counter3 / len(val_generator.classes)\n",
    "    return summ / len(val_generator.classes)\n",
    "c1 = x_difference(val_generator,10)\n",
    "\n",
    "print(c1)\n",
    "#print(c2)\n",
    "#print(c3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model64.save_weights(os.path.join(\"models\", \"Tentativa.h5\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "nb_class = 101\n",
    "\n",
    "path_dataset = \"../../dataset/FGNET/images/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ f for f in listdir(path_dataset) if isfile(join(path_dataset,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "x_batch = [cv2.resize(cv2.cvtColor(cv2.imread(path_dataset+img),cv2.COLOR_BGR2RGB),(224,224)) for img in images]\n",
    "\n",
    "y_true = [int(name.split('A')[1][:2]) for name in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(x_batch, y_true))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "x_batch, y_true = zip(*c)\n",
    "\n",
    "x_batch = np.array(x_batch)\n",
    "\n",
    "\n",
    "percentage = 0.2\n",
    "x_data = (x_batch[:int(len(x_batch)*(1-percentage))],x_batch[int(len(x_batch)*(1-percentage)):])\n",
    "y_data = (y_true[:int(len(y_true)*(1-percentage))],y_true[int(len(y_true)*(1-percentage)):])\n",
    "\n",
    "x_train, y_train = x_data[0], np.array(y_data[0])\n",
    "x_val, y_val = x_data[1], np.array(y_data[1])\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(y_val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and val\n",
    "\n",
    "y_train2 = to_categorical(y_train, nb_class).astype('float32')\n",
    "y_val2 = to_categorical(y_val, nb_class).astype('float32')\n",
    "\n",
    "y_train = y_train.reshape(len(y_train),1)\n",
    "y_val = y_val.reshape(len(y_val),1)\n",
    "\n",
    "y_train = np.array(y_train).reshape(-1)\n",
    "y_val = np.array(y_val).reshape(-1)\n",
    "#print(y_train2.shape)\n",
    "#print(x_val.shape)\n",
    "\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training data:  4492\n",
      "Size of Validation data:  1122\n"
     ]
    }
   ],
   "source": [
    "images = [ f for f in listdir(path_dataset) if isfile(join(path_dataset,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "num_val = int(len(images)*0.2)\n",
    "\n",
    "train_names = images[num_val:]\n",
    "val_names = images[:num_val]\n",
    "\n",
    "print(\"Size of Training data: \", len(train_names))\n",
    "print(\"Size of Validation data: \", len(val_names))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(list_filenames):\n",
    "    while True:\n",
    "        for batches in range(len(list_filenames) // batch_size):\n",
    "            x_batch = []\n",
    "            real_age = []\n",
    "            categorical_age = []\n",
    "            for filename in list_filenames[batches*batch_size:min(len(list_filenames),(batches+1)*batch_size)]:\n",
    "            #get real age\n",
    "                #age = int(filename.split('A')[1][:-4])\n",
    "                age = int(filename.split('A')[1][:2])\n",
    "                real_age.append(age)\n",
    "                #categorical\n",
    "                cat_age = to_categorical(age,nb_class)\n",
    "                categorical_age.append(cat_age)\n",
    "                #get image, color change and resize -> not needed, organize_db does this\n",
    "                image = cv2.imread(path_dataset+filename)\n",
    "                #image = cv2.resize(image,(224,224))\n",
    "                x_batch.append(image)\n",
    "                #print(image.shape)\n",
    "            x_batch = np.array(x_batch)\n",
    "            real_age = np.array(real_age)\n",
    "            categorical_age = np.array(categorical_age)\n",
    "            #print(x_batch.shape)\n",
    "            #print(real_age.shape)\n",
    "            #print(categorical_age.shape)\n",
    "            yield x_batch,[categorical_age,real_age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting...\n",
      "Epoch 1/10\n",
      "  6/140 [>.............................] - ETA: 5:22 - loss: 391.0564 - pred_age_loss: 14.6102 - regress_age_loss: 377.2337 - pred_age_acc: 0.0312 - regress_age_acc: 0.0365"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0bf46afa2779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m model.fit_generator(get_batch(train_names),\n\u001b[0;32m---> 16\u001b[0;31m                     steps_per_epoch=len(train_names) // batch_size, epochs=10, validation_data = get_batch(val_names),validation_steps=len(val_names))\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###\n",
    "from MyModel import MyModel\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "model = MyModel(224,nb_class=nb_class,trainable=False)()\n",
    "sgd = Adam(lr=0.0001)\n",
    "\n",
    "#getBatches\n",
    "\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Start Fitting...\")\n",
    "\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=10, validation_data = get_batch(val_names),validation_steps=len(val_names))\n",
    "sgd = Adam(lr=0.00001)\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=5, validation_data = get_batch(val_names),validation_steps=len(val_names))\n",
    "sgd = Adam(lr=0.000001)\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=5, validation_data = get_batch(val_names),validation_steps=len(val_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"../../dataset/FGNET_TEST/\"\n",
    "\n",
    "images = [ f for f in listdir(path_test) if isfile(join(path_test,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "x_test = [cv2.resize(cv2.cvtColor(cv2.imread(path_fg_net+img),cv2.COLOR_BGR2RGB),(224,224)) for img in images]\n",
    "\n",
    "y_test = [int(name.split('A')[1][:2]) for name in images]\n",
    "\n",
    "y_pred = model.predict(np.array(x_test))\n",
    "\n",
    "#for img in x_test[:10]:\n",
    "#    plt.figure()\n",
    "#    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.arange(0, nb_class).reshape(nb_class, 1)\n",
    "predicted_age = y_pred[0].dot(ages).flatten()\n",
    "predicted_age_regressed = y_pred[1]\n",
    "\n",
    "avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(x_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ1 = 0\n",
    "summ2 = 0\n",
    "summ3 = 0\n",
    "for i in range(len(x_test)):\n",
    "    summ1 += abs(y_test[i] - avg[i])\n",
    "    summ2 += abs(y_test[i] - predicted_age[i])\n",
    "    summ3 += abs(y_test[i] - predicted_age_regressed[i])\n",
    "\n",
    "print(summ1/len(x_test), \" -- AVG\")\n",
    "print(summ2/len(x_test), \" -- CLASS\")\n",
    "print(summ3/len(x_test), \" -- REG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/state_art3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import face_recognition\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(\"predictor/shape_predictor_5_face_landmarks.dat\")\n",
    "\n",
    "mypath='test/'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "#images32 = np.empty(len(onlyfiles), dtype=object)\n",
    "#faces32 = np.empty((len(onlyfiles), img_size32, img_size32, 3))\n",
    "images64 = np.empty(len(onlyfiles), dtype=object)\n",
    "faces64 = np.empty((len(onlyfiles), 224, 224, 3))\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   # images32[n] = cv2.imread( join(mypath,onlyfiles[n]))\n",
    "    #images32[n] = cv2.cvtColor(images32[n],cv2.COLOR_BGR2RGB)\n",
    "    #faces32[n, : , : , :] = cv2.resize(images32[n][35:-35,35:-35,:], (img_size32, img_size32))\n",
    "    image = face_recognition.load_image_file(join(mypath,onlyfiles[n]))\n",
    "    \n",
    "    dets = detector(image, 1)\n",
    "    \n",
    "    if len(dets) == 0:\n",
    "        continue\n",
    "    \n",
    "    faces = dlib.full_object_detections()\n",
    "    for detection in dets:\n",
    "        faces.append(sp(image, detection))\n",
    "    \n",
    "    images = dlib.get_face_chips(image, faces, size=320, padding=0.5)\n",
    "    \n",
    "    images64[n] = images[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(face_locations)\n",
    "    #print(face_locations[0],face_locations[2])\n",
    "    #print(face_locations[3],face_locations[1])\n",
    "    #print(image.shape)\n",
    "    #images64[n] = image[face_locations[0]:face_locations[2],face_locations[3]:face_locations[1],:]\n",
    "    faces64[n, : , : , :] = cv2.resize(images64[n], (224, 224))\n",
    "    \n",
    "#results32 = model32.predict(faces32)\n",
    "y_pred = model.predict(faces64)\n",
    "\n",
    "ages = np.arange(0, nb_class).reshape(nb_class, 1)\n",
    "predicted_age = y_pred[0].dot(ages).flatten()\n",
    "predicted_age_regressed = y_pred[1]\n",
    "\n",
    "avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(onlyfiles))]\n",
    "\n",
    "for n in range(len(onlyfiles)):\n",
    "    plt.figure()\n",
    "   # plt.imshow(cv2.resize(images32[n], (img_size64, img_size64)))\n",
    "    #print(\"Age \" +onlyfiles[n]+ \" (32x32): \", int(predicted_ages32[n]))\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (32x32): \", \"F\" if predicted_genders32[n][0] > 0.5 else \"M\")\n",
    "    plt.figure()\n",
    "    plt.imshow(images64[n])\n",
    "    print(\"AVG \" +onlyfiles[n]+ \" (64x64): \", avg[n])\n",
    "    print(\"Class \" +onlyfiles[n]+ \" (64x64): \", predicted_age[n])\n",
    "    print(\"Regress \" +onlyfiles[n]+ \" (64x64): \", predicted_age_regressed[n])\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (64x64): \", \"F\" if predicted_genders64[n][0] > 0.5 else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
