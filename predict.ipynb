{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from wide_resnet import WideResNet\n",
    "from MyModel import MyModel\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.applications import vgg16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# load model and weights\n",
    "img_size = 224\n",
    "#model64 = MyModel(img_size)()\n",
    "#model64.load_weights(os.path.join(\"models\", \"WRN_16_8.h5\"))\n",
    "\n",
    "#img_size32 = 32\n",
    "#model32 = WideResNet(img_size32, depth=16, k=8)()\n",
    "#model32.load_weights(os.path.join(\"../../dataset/checkpoints\", \"model32.hdf5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"wiki\"\n",
    "data = scipy.io.loadmat(os.path.join(\"../../dataset/\", \"{}32.mat\".format(db)))\n",
    "test_images = data[\"image\"]\n",
    "test_genders = data[\"gender\"][0]\n",
    "test_ages = data[\"age\"][0]\n",
    "\n",
    "cols, rows = 4, 3\n",
    "img_num = cols * rows\n",
    "path_root = \"data/{}_crop/\".format(db)\n",
    "img_ids = np.random.choice(len(test_ages), img_num)\n",
    "sub_test_images = test_images[img_ids,:,:,:]\n",
    "\n",
    "# predict\n",
    "results = model32.predict(sub_test_images)\n",
    "predicted_genders = results[0]\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "for i in range(img_num):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(cv2.cvtColor(sub_test_images[i,:,:,:], cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"{}, {}\".format(int(predicted_ages[i]),\n",
    "                              \"F\" if predicted_genders[i][0]>0.5 else \"M\"))\n",
    "    plt.axis('off')\n",
    "plt.savefig(\"result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import face_recognition\n",
    "mypath='test/'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "#images32 = np.empty(len(onlyfiles), dtype=object)\n",
    "#faces32 = np.empty((len(onlyfiles), img_size32, img_size32, 3))\n",
    "images64 = np.empty(len(onlyfiles), dtype=object)\n",
    "faces64 = np.empty((len(onlyfiles), 224, 224, 3))\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   # images32[n] = cv2.imread( join(mypath,onlyfiles[n]))\n",
    "    #images32[n] = cv2.cvtColor(images32[n],cv2.COLOR_BGR2RGB)\n",
    "    #faces32[n, : , : , :] = cv2.resize(images32[n][35:-35,35:-35,:], (img_size32, img_size32))\n",
    "    image = face_recognition.load_image_file(join(mypath,onlyfiles[n]))\n",
    "    face_locations = face_recognition.face_locations(image)[0]\n",
    "    #print(face_locations)\n",
    "    #print(face_locations[0],face_locations[2])\n",
    "    #print(face_locations[3],face_locations[1])\n",
    "    #print(image.shape)\n",
    "    images64[n] = image[face_locations[0]:face_locations[2],face_locations[3]:face_locations[1],:]\n",
    "    faces64[n, : , : , :] = cv2.resize(images64[n], (224, 224))\n",
    "    \n",
    "#results32 = model32.predict(faces32)\n",
    "results64 = model64.predict(faces64)\n",
    "#predicted_genders32 = results32[0]\n",
    "#predicted_genders64 = results64[0]\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "#predicted_ages32 = results32[1].dot(ages).flatten()\n",
    "predicted_ages64 = results64.dot(ages).flatten()\n",
    "\n",
    "for n in range(len(onlyfiles)):\n",
    "    plt.figure()\n",
    "   # plt.imshow(cv2.resize(images32[n], (img_size64, img_size64)))\n",
    "    #print(\"Age \" +onlyfiles[n]+ \" (32x32): \", int(predicted_ages32[n]))\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (32x32): \", \"F\" if predicted_genders32[n][0] > 0.5 else \"M\")\n",
    "    plt.figure()\n",
    "    plt.imshow(images64[n])\n",
    "    print(\"Age \" +onlyfiles[n]+ \" (64x64): \", predicted_ages64[n])\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (64x64): \", \"F\" if predicted_genders64[n][0] > 0.5 else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def generate_data_generator(generator):\n",
    "    for x_batch,y_batch in generator:\n",
    "            ages = np.arange(0, 101).reshape(101, 1)\n",
    "            round_age = [int(pred.dot(ages).flatten()) for pred in y_batch]\n",
    "            yield x_batch, [y_batch, np.array(round_age)]\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    '../../dataset/wiki_crop/new_database/',\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def x_difference(val_generator, X):\n",
    "    print(len(val_generator.classes))\n",
    "    predictions = model64.predict_generator(generate_data_generator(val_generator),steps=int(6769 / 16))\n",
    "    print(\"Done Prediction...\")\n",
    "    ages = np.arange(0, 101).reshape(101, 1)\n",
    "    predicted_age = predictions[0].dot(ages).flatten()\n",
    "    predicted_age_regressed = predictions[1]\n",
    "    \n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    counter3 = 0\n",
    "    #print(predicted_age_regressed)\n",
    "    \n",
    "    avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(val_generator.classes))]\n",
    "    \n",
    "    #print(avg)\n",
    "    summ = 0\n",
    "    for i in range(len(val_generator.classes)):\n",
    "        real_label = val_generator.classes[i]\n",
    "        #print(real_label)\n",
    "        #print(avg[i])\n",
    "        if abs(real_label - predicted_age[i]) <= X:\n",
    "            counter += 1\n",
    "        if abs(real_label - avg[i]) <= X:\n",
    "            counter2 += 1\n",
    "        if abs(real_label - predicted_age_regressed[i]) <= X:\n",
    "            counter3 += 1\n",
    "        \n",
    "        summ += abs(real_label - avg[i])\n",
    "    #return counter / len(val_generator.classes),counter2 / len(val_generator.classes), counter3 / len(val_generator.classes)\n",
    "    return summ / len(val_generator.classes)\n",
    "c1 = x_difference(val_generator,10)\n",
    "\n",
    "print(c1)\n",
    "#print(c2)\n",
    "#print(c3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model64.save_weights(os.path.join(\"models\", \"Tentativa.h5\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "nb_class = 101\n",
    "\n",
    "path_dataset = \"../../dataset/FGNET/images/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ f for f in listdir(path_dataset) if isfile(join(path_dataset,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "x_batch = [cv2.resize(cv2.cvtColor(cv2.imread(path_dataset+img),cv2.COLOR_BGR2RGB),(224,224)) for img in images]\n",
    "\n",
    "y_true = [int(name.split('A')[1][:2]) for name in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(x_batch, y_true))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "x_batch, y_true = zip(*c)\n",
    "\n",
    "x_batch = np.array(x_batch)\n",
    "\n",
    "\n",
    "percentage = 0.2\n",
    "x_data = (x_batch[:int(len(x_batch)*(1-percentage))],x_batch[int(len(x_batch)*(1-percentage)):])\n",
    "y_data = (y_true[:int(len(y_true)*(1-percentage))],y_true[int(len(y_true)*(1-percentage)):])\n",
    "\n",
    "x_train, y_train = x_data[0], np.array(y_data[0])\n",
    "x_val, y_val = x_data[1], np.array(y_data[1])\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(y_val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and val\n",
    "\n",
    "y_train2 = to_categorical(y_train, nb_class).astype('float32')\n",
    "y_val2 = to_categorical(y_val, nb_class).astype('float32')\n",
    "\n",
    "y_train = y_train.reshape(len(y_train),1)\n",
    "y_val = y_val.reshape(len(y_val),1)\n",
    "\n",
    "y_train = np.array(y_train).reshape(-1)\n",
    "y_val = np.array(y_val).reshape(-1)\n",
    "#print(y_train2.shape)\n",
    "#print(x_val.shape)\n",
    "\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ f for f in listdir(path_dataset) if isfile(join(path_dataset,f)) ]\n",
    "\n",
    "images = sorted(images)\n",
    "\n",
    "num_val = int(len(images)*0.2)\n",
    "\n",
    "train_names = images[num_val:]\n",
    "val_names = images[:num_val]\n",
    "\n",
    "print(\"Size of Training data: \", len(train_names))\n",
    "print(\"Size of Validation data: \", len(val_names))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(list_filenames):\n",
    "    while True:\n",
    "        for batches in range(len(list_filenames) // batch_size):\n",
    "            x_batch = []\n",
    "            real_age = []\n",
    "            categorical_age = []\n",
    "            for filename in list_filenames[batches*batch_size:min(len(list_filenames),(batches+1)*batch_size)]:\n",
    "            #get real age\n",
    "                #age = int(filename.split('A')[1][:-4])\n",
    "                age = int(filename.split('A')[1][:2])\n",
    "                real_age.append(age)\n",
    "                #categorical\n",
    "                cat_age = to_categorical(age,nb_class)\n",
    "                categorical_age.append(cat_age)\n",
    "                #get image, color change and resize -> not needed, organize_db does this\n",
    "                image = cv2.imread(path_dataset+filename)\n",
    "                if image.shape != (224,224,3):\n",
    "                    image = cv2.resize(image,(224,224))\n",
    "                x_batch.append(image)\n",
    "                #print(image.shape)\n",
    "            x_batch = np.array(x_batch)\n",
    "            real_age = np.array(real_age)\n",
    "            categorical_age = np.array(categorical_age)\n",
    "            #print(x_batch.shape)\n",
    "            #print(real_age.shape)\n",
    "            #print(categorical_age.shape)\n",
    "            yield x_batch,[categorical_age,real_age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "import keras\n",
    "from MyModel import MyModel\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "model = MyModel(224,nb_class=nb_class,trainable=False)()\n",
    "sgd = Adam(lr=0.0001)\n",
    "\n",
    "#getBatches\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "tbCallBack1 = keras.callbacks.TensorBoard(log_dir='../../dataset/graphs/Graph1', histogram_freq=0, write_graph=True, write_images=True)\n",
    "tbCallBack2 = keras.callbacks.TensorBoard(log_dir='../../dataset/graphs/Graph2', histogram_freq=0, write_graph=True, write_images=True)\n",
    "tbCallBack3 = keras.callbacks.TensorBoard(log_dir='../../dataset/graphs/Graph3', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy','MAE'])\n",
    "\n",
    "print(\"Start Fitting...\")\n",
    "\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=25, validation_data = get_batch(val_names),validation_steps=len(val_names)// batch_size, callbacks=[tbCallBack1])\n",
    "\n",
    "sgd = Adam(lr=0.00001)\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=5, validation_data = get_batch(val_names),validation_steps=len(val_names)// batch_size, callbacks=[tbCallBack2])\n",
    "sgd = Adam(lr=0.000001)\n",
    "model.compile(optimizer=sgd, loss=[\"categorical_crossentropy\",\"MSE\"], loss_weights=[0.5,1],\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(get_batch(train_names),\n",
    "                    steps_per_epoch=len(train_names) // batch_size, epochs=5, validation_data = get_batch(val_names),validation_steps=len(val_names)// batch_size, callbacks=[tbCallBack3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "model = load_model('../../dataset/models/state_meh.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.models import load_model\n",
    "\n",
    "path_test = \"../../dataset/FGNET/images/\"\n",
    "\n",
    "images = [ f for f in listdir(path_test)[0:300] if isfile(join(path_test,f)) ]\n",
    "\n",
    "x_test = [cv2.cvtColor(cv2.resize(cv2.imread(path_test+img),(224,224)),cv2.COLOR_BGR2RGB) for img in images]\n",
    "\n",
    "y_test = [int(name.split('A')[1][:2]) for name in images]\n",
    "\n",
    "model = load_model(\"../thesis/code/models/files/keras_model.h5\")\n",
    "\n",
    "y_pred = model.predict(np.array(x_test))\n",
    "\n",
    "#for img in x_test[:10]:\n",
    "#    plt.figure()\n",
    "#    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.27633154 30.2464746  26.35517326 23.7574832  24.23744669 14.54653027\n",
      " 26.43840288 41.90188334 35.03860332 31.69120036 35.10794889 27.13197623\n",
      " 27.72982554 30.81158072 18.83678941 47.14978933 44.61677047 22.35714991\n",
      " 16.47947268 42.18180727 31.85462914 21.28834067 36.2297036  40.72183454\n",
      " 22.88967024 20.15650082 26.08446028 34.8167525  39.835358   26.06633367\n",
      " 21.53401258 29.95619355 16.2150328  30.4102748  28.61440934 20.76262879\n",
      " 44.84121111 61.21186871 36.29431954 21.83498569 35.21746902 41.46919639\n",
      " 44.19832021 21.11095361 29.40122794 19.29188443 21.57340266 16.90774834\n",
      " 15.51636079 21.96714136 54.75166599 34.00312861 27.21998956 21.31052181\n",
      " 18.97744893 32.71521947 29.05602298 22.43472602 18.83185201 15.15858603\n",
      " 21.38835369 17.98902531 15.4563696  37.79873703 26.01330618 20.34915912\n",
      " 33.96271885 28.39667363 18.45909975 25.4449587  32.16749251 21.28473873\n",
      " 18.72752195 33.11896701 42.35275807 17.43351071 29.25903113 31.65905918\n",
      " 27.37554289 22.95834111 20.50423018 29.59722548 30.53243988 22.58591194\n",
      " 20.58940758 22.71861808 27.86576459 33.39751633 42.98907019 20.91404215\n",
      " 26.89537822 23.52212669 41.66952035 23.12251141 35.96756291 21.51455828\n",
      " 26.00250183 25.09645571 18.65698886 31.19092094 18.21209874 30.21210518\n",
      " 23.62491804 21.38481172 47.52231276 46.88202603 24.71234556 38.26818338\n",
      " 24.02073532 29.8965302  49.98353372 15.51415727 17.24522673 22.13741209\n",
      " 20.62970529 34.75922586 17.373494   49.86814077 18.41171191 19.47438156\n",
      " 17.70267318 26.0018003  55.10405141 28.66758019 20.1233188  27.15419949\n",
      " 24.92687961 15.61321139 38.10514276 24.20177587 31.62681294 15.83942442\n",
      " 16.29294223 41.20072222 48.44321983 43.27179838 30.19698951 37.34908942\n",
      " 46.67411019 20.21449104 26.54764894 32.09604341 23.51236851 23.53492821\n",
      " 22.69618444 29.16458637 29.74150917 34.39259406 26.23835526 16.47152418\n",
      " 33.71423236 40.89561756 39.68304615 21.42561021 36.9135644  18.58281232\n",
      " 24.6991154  22.26499861 18.76151018 23.84677026 24.03075264 23.96964716\n",
      " 26.80272103 42.31749102 23.7963212  18.6668462  23.12554598 15.77385466\n",
      " 18.68663113 35.69269405 42.72843593 23.36109963 42.52751494 22.78912408\n",
      " 18.97193382 14.60305991 16.72804386 35.00303443 19.1844     22.23905969\n",
      " 20.13164619 15.50428257 22.01044605 14.91253406 44.20383547 22.52451398\n",
      " 41.85241413 27.41639166 33.90285341 37.53634202 31.9945772  26.24039401\n",
      " 30.56081771 29.76628844 19.23968505 22.45279904 21.3288854  16.78879669\n",
      " 23.47757877 31.59141527 30.15399353 33.40381799 42.43693524 22.53828961\n",
      " 18.13519417 18.09208528 23.84213404 25.77521251 24.50109106 30.46347298\n",
      " 29.58506928 30.03769645 32.65874798 34.72159143 28.57176409 22.45388819\n",
      " 21.53048432 16.72858001 30.16843427 33.50924119 25.74694337 19.86260852\n",
      " 28.51702372 36.68290985 40.28596833 20.62364439 28.17208574 41.15034308\n",
      " 20.72014572 26.09495638 16.418963   25.89047843 20.77092595 31.91969586\n",
      " 25.90013468 20.75203454 25.384457   24.97762256 44.39879836 20.98516528\n",
      " 20.20576625 33.24051015 18.20083731 30.66416914 25.74497319 25.64346178\n",
      " 28.1496932  23.99723223 27.08436984 20.64818673 17.81713862 18.6070192\n",
      " 24.61454005 30.13493983 30.50444474 20.3401785  21.01197533 21.1241259\n",
      " 46.01757283 32.46876911 37.44061698 19.17044758 16.54352317 21.42326395\n",
      " 30.36604231 30.36973492 16.4319929  33.56906372 33.59136817 33.10164356\n",
      " 16.06972405 42.40900326 32.34832621 50.90761861 31.97163804 39.81286182\n",
      " 18.03123601 17.8600958  31.21222125 31.17354    18.1244851  26.92911311\n",
      " 20.74496204 19.31536772 45.07687284 30.76468257 15.37377915 24.89813117\n",
      " 23.8869592  20.87506861 25.95166454 28.00669365 25.12947649 23.65278953\n",
      " 50.91144187 22.48691995 36.37539579 33.04917616 23.57218258 32.0786799 ]\n"
     ]
    }
   ],
   "source": [
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "predicted_age = y_pred.dot(ages).flatten()\n",
    "#predicted_age_regressed = y_pred[1]\n",
    "print(predicted_age)\n",
    "#avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(x_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.738751875551685  -- CLASS\n"
     ]
    }
   ],
   "source": [
    "#summ1 = 0\n",
    "summ2 = 0\n",
    "#summ3 = 0\n",
    "for i in range(len(x_test)):\n",
    "    #summ1 += abs(y_test[i] - avg[i])\n",
    "    summ2 += abs(y_test[i] - predicted_age[i])\n",
    "    ##summ3 += abs(y_test[i] - predicted_age_regressed[i])\n",
    "\n",
    "#print(summ1/len(x_test), \" -- AVG\")\n",
    "print(summ2/len(x_test), \" -- CLASS\")\n",
    "#print(summ3/len(x_test), \" -- REG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../dataset/models/state_art_maybe.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import face_recognition\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(\"predictor/shape_predictor_5_face_landmarks.dat\")\n",
    "\n",
    "mypath='test/'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "#images32 = np.empty(len(onlyfiles), dtype=object)\n",
    "#faces32 = np.empty((len(onlyfiles), img_size32, img_size32, 3))\n",
    "images64 = np.empty(len(onlyfiles), dtype=object)\n",
    "faces64 = np.empty((len(onlyfiles), 224, 224, 3))\n",
    "for n in range(0, len(onlyfiles)):\n",
    "   # images32[n] = cv2.imread( join(mypath,onlyfiles[n]))\n",
    "    #images32[n] = cv2.cvtColor(images32[n],cv2.COLOR_BGR2RGB)\n",
    "    #faces32[n, : , : , :] = cv2.resize(images32[n][35:-35,35:-35,:], (img_size32, img_size32))\n",
    "    image = face_recognition.load_image_file(join(mypath,onlyfiles[n]))\n",
    "    \n",
    "    dets = detector(image, 1)\n",
    "    \n",
    "    if len(dets) == 0:\n",
    "        continue\n",
    "    \n",
    "    faces = dlib.full_object_detections()\n",
    "    for detection in dets:\n",
    "        faces.append(sp(image, detection))\n",
    "    \n",
    "    images = dlib.get_face_chips(image, faces, size=320, padding=0.5)\n",
    "    \n",
    "    images64[n] = images[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(face_locations)\n",
    "    #print(face_locations[0],face_locations[2])\n",
    "    #print(face_locations[3],face_locations[1])\n",
    "    #print(image.shape)\n",
    "    #images64[n] = image[face_locations[0]:face_locations[2],face_locations[3]:face_locations[1],:]\n",
    "    faces64[n, : , : , :] = cv2.resize(images64[n], (224, 224))\n",
    "    \n",
    "#results32 = model32.predict(faces32)\n",
    "y_pred = model.predict(faces64)\n",
    "\n",
    "ages = np.arange(0, nb_class).reshape(nb_class, 1)\n",
    "predicted_age = y_pred[0].dot(ages).flatten()\n",
    "predicted_age_regressed = y_pred[1]\n",
    "\n",
    "avg = [(predicted_age[i] + predicted_age_regressed[i])/2 for i in range(len(onlyfiles))]\n",
    "\n",
    "for n in range(len(onlyfiles)):\n",
    "    plt.figure()\n",
    "   # plt.imshow(cv2.resize(images32[n], (img_size64, img_size64)))\n",
    "    #print(\"Age \" +onlyfiles[n]+ \" (32x32): \", int(predicted_ages32[n]))\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (32x32): \", \"F\" if predicted_genders32[n][0] > 0.5 else \"M\")\n",
    "    plt.figure()\n",
    "    plt.imshow(images64[n])\n",
    "    print(\"AVG \" +onlyfiles[n]+ \" (64x64): \", avg[n])\n",
    "    print(\"Class \" +onlyfiles[n]+ \" (64x64): \", predicted_age[n])\n",
    "    print(\"Regress \" +onlyfiles[n]+ \" (64x64): \", predicted_age_regressed[n])\n",
    "    #print(\"Gender \" +onlyfiles[n]+ \" (64x64): \", \"F\" if predicted_genders64[n][0] > 0.5 else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
